{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 10: Time-Affinity Optimization\n",
    "\n",
    "This notebook demonstrates walltime-based parameter discovery and optimization.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Use execution time as a fitness signal\n",
    "- Discover optimal parameters empirically\n",
    "- Diagnostic tool for unknown relationships\n",
    "- Correct parameters → Less work → Faster execution\n",
    "\n",
    "**Use Case**: When you don't know the optimal parameter values but suspect that correct values will make the algorithm run faster.\n",
    "\n",
    "Run all cells to see outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from workbench import (\n",
    "    TimeAffinityOptimizer,\n",
    "    GridSearchTimeAffinity,\n",
    "    quick_calibrate,\n",
    "    TimeAffinityResult\n",
    ")\n",
    "\n",
    "print('=' * 70)\n",
    "print('DEMO 10: TIME-AFFINITY OPTIMIZATION')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Concept - Finding Optimal Complexity\n",
    "\n",
    "Suppose we have an algorithm with unknown optimal parameters. We can use time-affinity to discover them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm with hidden optimal parameters\n",
    "def mystery_algorithm(threshold, iterations):\n",
    "    \"\"\"\n",
    "    This algorithm has a 'sweet spot' where it runs fastest.\n",
    "    Optimal: threshold ≈ 0.5, iterations ≈ 10\n",
    "    \"\"\"\n",
    "    result = 0\n",
    "    \n",
    "    # Inefficient if threshold too high or too low\n",
    "    penalty = abs(threshold - 0.5) * 1000\n",
    "    \n",
    "    # Inefficient if iterations far from optimal\n",
    "    penalty += abs(iterations - 10) * 50\n",
    "    \n",
    "    for i in range(int(100 + penalty)):\n",
    "        result += np.sum(np.random.rand(50))\n",
    "    \n",
    "    return result\n",
    "\n",
    "print('Mystery Algorithm: Unknown optimal parameters')\n",
    "print('Goal: Discover parameters that minimize execution time')\n",
    "print()\n",
    "\n",
    "# Test a few random configurations\n",
    "print('Testing random configurations:')\n",
    "for threshold, iterations in [(0.1, 5), (0.5, 10), (0.9, 20)]:\n",
    "    start = time.perf_counter()\n",
    "    mystery_algorithm(threshold, iterations)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f'  threshold={threshold:.1f}, iterations={iterations:2d}: {elapsed:.6f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use time-affinity to discover optimal parameters\n",
    "print('\\nUsing Time-Affinity to discover optimal parameters:')\n",
    "print('-' * 70)\n",
    "\n",
    "# We'll target a fast time (e.g., 0.01s) and let the optimizer find params\n",
    "result = quick_calibrate(\n",
    "    mystery_algorithm,\n",
    "    target_time=0.01,\n",
    "    param_bounds={\n",
    "        'threshold': (0.0, 1.0),\n",
    "        'iterations': (1.0, 30.0)\n",
    "    },\n",
    "    method='gradient',\n",
    "    max_iterations=30,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f'\\nDiscovered optimal parameters:')\n",
    "print(f'  threshold:  {result.best_params[\"threshold\"]:.3f} (true optimal: 0.500)')\n",
    "print(f'  iterations: {result.best_params[\"iterations\"]:.1f} (true optimal: 10.0)')\n",
    "print(f'\\nAchieved time: {result.best_time:.6f}s')\n",
    "print(f'Iterations: {result.iterations}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Diagnostic Use - Discovering Relationships\n",
    "\n",
    "Time-affinity can reveal unknown parameter relationships by observing which combinations run fastest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm with hidden relationship: optimal when x * y ≈ 10\n",
    "def coupled_algorithm(x, y):\n",
    "    \"\"\"\n",
    "    Hidden relationship: x * y should equal 10 for optimal performance\n",
    "    \"\"\"\n",
    "    product = x * y\n",
    "    penalty = abs(product - 10) * 100\n",
    "    \n",
    "    for i in range(int(100 + penalty)):\n",
    "        _ = np.sum(np.random.rand(30))\n",
    "    \n",
    "    return product\n",
    "\n",
    "print('Coupled Algorithm: Parameters have hidden relationship')\n",
    "print('Goal: Discover the relationship x * y = 10')\n",
    "print()\n",
    "\n",
    "# Use grid search to map the parameter space\n",
    "optimizer = GridSearchTimeAffinity(\n",
    "    target_time=0.005,\n",
    "    param_grids={\n",
    "        'x': np.linspace(1, 20, 10),\n",
    "        'y': np.linspace(0.5, 10, 10)\n",
    "    },\n",
    "    warmup_runs=2\n",
    ")\n",
    "\n",
    "result = optimizer.optimize(coupled_algorithm, verbose=False)\n",
    "\n",
    "print(f'Discovered parameters:')\n",
    "print(f'  x = {result.best_params[\"x\"]:.2f}')\n",
    "print(f'  y = {result.best_params[\"y\"]:.2f}')\n",
    "print(f'  x * y = {result.best_params[\"x\"] * result.best_params[\"y\"]:.2f} (target: 10.0)')\n",
    "print(f'\\nTime: {result.best_time:.6f}s')\n",
    "print(f'\\n✓ Discovered the hidden relationship empirically!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Real-World Example - SRT Parameter Discovery\n",
    "\n",
    "Discover optimal SRT-style parameters for a spectral algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workbench import SpectralScorer, ZetaFiducials\n",
    "\n",
    "# Create a spectral scoring task\n",
    "zeros = ZetaFiducials.get_standard(20)\n",
    "candidates = np.arange(1000, 2000)\n",
    "\n",
    "def spectral_task(damping, shift):\n",
    "    \"\"\"\n",
    "    Spectral scoring with variable parameters.\n",
    "    Optimal parameters should minimize computation.\n",
    "    \"\"\"\n",
    "    scorer = SpectralScorer(frequencies=zeros, damping=damping)\n",
    "    scores = scorer.compute_scores(candidates, shift=shift, mode='real')\n",
    "    return scores\n",
    "\n",
    "print('Real-World: Discovering optimal spectral parameters')\n",
    "print('-' * 70)\n",
    "\n",
    "# Find parameters that make spectral scoring run in ~0.05s\n",
    "result = quick_calibrate(\n",
    "    spectral_task,\n",
    "    target_time=0.05,\n",
    "    param_bounds={\n",
    "        'damping': (0.01, 0.2),\n",
    "        'shift': (0.0, 0.1)\n",
    "    },\n",
    "    method='gradient',\n",
    "    max_iterations=20,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f'Discovered parameters:')\n",
    "print(f'  damping: {result.best_params[\"damping\"]:.4f}')\n",
    "print(f'  shift:   {result.best_params[\"shift\"]:.4f}')\n",
    "print(f'\\nExecution time: {result.best_time:.6f}s')\n",
    "print(f'Target time:    {result.target_time:.6f}s')\n",
    "print(f'Error:          {result.time_error:.6f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze convergence behavior\n",
    "def analysis_algorithm(alpha, beta):\n",
    "    penalty = (alpha - 0.3)**2 + (beta - 0.7)**2\n",
    "    for i in range(int(50 + penalty * 1000)):\n",
    "        _ = np.sum(np.random.rand(20))\n",
    "    return alpha + beta\n",
    "\n",
    "print('Convergence Analysis:')\n",
    "print('-' * 70)\n",
    "\n",
    "optimizer = TimeAffinityOptimizer(\n",
    "    target_time=0.003,\n",
    "    param_bounds={'alpha': (0.0, 1.0), 'beta': (0.0, 1.0)},\n",
    "    max_iterations=30,\n",
    "    learning_rate=0.1,\n",
    "    momentum=0.5\n",
    ")\n",
    "\n",
    "result = optimizer.optimize(analysis_algorithm, verbose=False)\n",
    "\n",
    "print(f'\\nFinal parameters:')\n",
    "print(f'  alpha: {result.best_params[\"alpha\"]:.3f} (optimal: 0.300)')\n",
    "print(f'  beta:  {result.best_params[\"beta\"]:.3f} (optimal: 0.700)')\n",
    "\n",
    "# Show convergence\n",
    "print(f'\\nTime evolution (first 10 iterations):')\n",
    "for i, t in enumerate(result.time_history[:10]):\n",
    "    error = abs(t - result.target_time)\n",
    "    print(f'  Iter {i:2d}: time={t:.6f}s, error={error:.6f}s')\n",
    "\n",
    "print(f'\\nConvergence rate: {result.convergence_rate:.6f}s/iter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Grid Search vs Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare methods\n",
    "def test_algorithm(p1, p2):\n",
    "    penalty = (p1 - 0.6)**2 + (p2 - 0.4)**2\n",
    "    for i in range(int(30 + penalty * 500)):\n",
    "        _ = np.sum(np.random.rand(15))\n",
    "    return p1 * p2\n",
    "\n",
    "print('Method Comparison: Grid Search vs Gradient Descent')\n",
    "print('=' * 70)\n",
    "\n",
    "# Gradient descent\n",
    "print('\\n1. Gradient Descent:')\n",
    "start = time.time()\n",
    "result_grad = quick_calibrate(\n",
    "    test_algorithm,\n",
    "    target_time=0.002,\n",
    "    param_bounds={'p1': (0.0, 1.0), 'p2': (0.0, 1.0)},\n",
    "    method='gradient',\n",
    "    max_iterations=20,\n",
    "    verbose=False\n",
    ")\n",
    "time_grad = time.time() - start\n",
    "\n",
    "print(f'   Found: p1={result_grad.best_params[\"p1\"]:.3f}, p2={result_grad.best_params[\"p2\"]:.3f}')\n",
    "print(f'   Time error: {result_grad.time_error:.6f}s')\n",
    "print(f'   Optimization time: {time_grad:.3f}s')\n",
    "\n",
    "# Grid search\n",
    "print('\\n2. Grid Search:')\n",
    "start = time.time()\n",
    "result_grid = quick_calibrate(\n",
    "    test_algorithm,\n",
    "    target_time=0.002,\n",
    "    param_bounds={'p1': (0.0, 1.0), 'p2': (0.0, 1.0)},\n",
    "    method='grid',\n",
    "    grid_points=10,\n",
    "    verbose=False\n",
    ")\n",
    "time_grid = time.time() - start\n",
    "\n",
    "print(f'   Found: p1={result_grid.best_params[\"p1\"]:.3f}, p2={result_grid.best_params[\"p2\"]:.3f}')\n",
    "print(f'   Time error: {result_grid.time_error:.6f}s')\n",
    "print(f'   Optimization time: {time_grid:.3f}s')\n",
    "\n",
    "print('\\nComparison:')\n",
    "print(f'   Gradient: {result_grad.iterations} evaluations in {time_grad:.3f}s')\n",
    "print(f'   Grid:     {result_grid.iterations} evaluations in {time_grid:.3f}s')\n",
    "print(f'   Speedup:  {time_grid/time_grad:.1f}x faster with gradient descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Diagnostic Use Case - Unknown Algorithm Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate discovering unknown algorithm behavior\n",
    "def black_box_algorithm(param_a, param_b, param_c):\n",
    "    \"\"\"\n",
    "    Complex algorithm with unknown optimal configuration.\n",
    "    Hidden rule: param_a + 2*param_b - param_c should equal 5\n",
    "    \"\"\"\n",
    "    target_value = 5\n",
    "    actual_value = param_a + 2*param_b - param_c\n",
    "    penalty = abs(actual_value - target_value) * 200\n",
    "    \n",
    "    for i in range(int(50 + penalty)):\n",
    "        _ = np.sum(np.random.rand(25))\n",
    "    \n",
    "    return actual_value\n",
    "\n",
    "print('Diagnostic Use Case: Discovering Unknown Relationships')\n",
    "print('=' * 70)\n",
    "print('\\nScenario: Black-box algorithm with unknown optimal parameters')\n",
    "print('Goal: Discover parameter values empirically using time as signal')\n",
    "print()\n",
    "\n",
    "# Use time-affinity to discover the relationship\n",
    "result = quick_calibrate(\n",
    "    black_box_algorithm,\n",
    "    target_time=0.003,\n",
    "    param_bounds={\n",
    "        'param_a': (0.0, 10.0),\n",
    "        'param_b': (0.0, 10.0),\n",
    "        'param_c': (0.0, 10.0)\n",
    "    },\n",
    "    method='gradient',\n",
    "    max_iterations=40,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "a = result.best_params['param_a']\n",
    "b = result.best_params['param_b']\n",
    "c = result.best_params['param_c']\n",
    "\n",
    "print(f'Discovered parameters:')\n",
    "print(f'  param_a = {a:.2f}')\n",
    "print(f'  param_b = {b:.2f}')\n",
    "print(f'  param_c = {c:.2f}')\n",
    "print(f'\\nDiscovered relationship:')\n",
    "print(f'  param_a + 2*param_b - param_c = {a + 2*b - c:.2f}')\n",
    "print(f'  (Hidden target value: 5.0)')\n",
    "print(f'\\n✓ Successfully discovered the hidden constraint empirically!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Time-Affinity Optimization** is a diagnostic tool for:\n",
    "\n",
    "**Primary Use Cases:**\n",
    "1. **Parameter Discovery**: Find optimal values when you don't know them\n",
    "2. **Relationship Discovery**: Reveal hidden parameter relationships\n",
    "3. **Empirical Calibration**: Tune algorithms without theoretical models\n",
    "4. **Performance Diagnosis**: Understand why certain configs run faster\n",
    "\n",
    "**Key Principle:**\n",
    "- Correct/resonant parameters → Algorithm does less work → Faster execution\n",
    "- Walltime becomes a proxy for parameter quality\n",
    "\n",
    "**When to Use:**\n",
    "- Unknown optimal parameter values\n",
    "- Suspected parameter relationships\n",
    "- Black-box algorithm tuning\n",
    "- Empirical performance analysis\n",
    "\n",
    "**Methods:**\n",
    "- **Gradient Descent**: Fast, good for continuous spaces\n",
    "- **Grid Search**: Exhaustive, good for discrete/complex spaces\n",
    "\n",
    "**Limitations:**\n",
    "- Requires algorithm to have time-parameter relationship\n",
    "- Timing noise can affect convergence\n",
    "- Local minima possible with gradient descent\n",
    "\n",
    "**Best Practices:**\n",
    "- Use multiple warmup runs to reduce noise\n",
    "- Start with grid search for exploration\n",
    "- Refine with gradient descent\n",
    "- Verify discovered parameters make sense"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
