{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 12: Error Pattern Visualizer\n",
    "\n",
    "This notebook demonstrates the **Error Pattern Visualizer** for automatically discovering correction patterns in error signals.\n",
    "\n",
    "## Why This Tool Exists\n",
    "\n",
    "During optimization of `fast_zetas.py`, we manually:\n",
    "- Computed error = actual - predicted\n",
    "- Analyzed error using FFT to find dominant frequencies\n",
    "- Fitted sinusoidal corrections\n",
    "- Applied recursive refinement to residuals\n",
    "\n",
    "This tool **automates all of that**. It discovers patterns you might not think of and generates production-ready correction code.\n",
    "\n",
    "## Key Insight\n",
    "\n",
    "**Errors are not random—they contain structure that reveals missing terms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from error_pattern_visualizer import (\n",
    "    ErrorPatternAnalyzer,\n",
    "    ErrorVisualizer\n",
    ")\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Error Analysis\n",
    "\n",
    "Analyze a simple error pattern: sin wave with missing linear trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "x = np.linspace(0, 10, 100)\n",
    "actual = np.sin(x) + 0.1 * x  # Sin + linear trend\n",
    "predicted = np.sin(x)  # Missing linear term\n",
    "\n",
    "# Analyze errors\n",
    "analyzer = ErrorPatternAnalyzer(actual, predicted, x, name=\"Sin + Linear\")\n",
    "report = analyzer.analyze_all()\n",
    "\n",
    "# Print summary\n",
    "report.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show generated correction code\n",
    "print(\"\\nTop correction code:\")\n",
    "print(\"=\" * 50)\n",
    "print(report.suggestions[0].code_snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spectral Pattern Detection\n",
    "\n",
    "Detect periodic patterns using FFT analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with multiple harmonics\n",
    "x = np.linspace(0, 10, 200)\n",
    "actual = (np.sin(2 * np.pi * 1.0 * x) + \n",
    "          0.5 * np.sin(2 * np.pi * 3.0 * x) + \n",
    "          0.3 * np.sin(2 * np.pi * 5.0 * x))\n",
    "predicted = np.sin(2 * np.pi * 1.0 * x)  # Missing harmonics\n",
    "\n",
    "analyzer = ErrorPatternAnalyzer(actual, predicted, x, name=\"Multiple Harmonics\")\n",
    "\n",
    "# Analyze spectral patterns\n",
    "spectral = analyzer.analyze_spectral(n_harmonics=5)\n",
    "print(f\"Detected {len(spectral.frequencies)} harmonics\")\n",
    "print(f\"Explained variance: {spectral.explained_variance:.1%}\")\n",
    "print(f\"\\nDominant frequencies:\")\n",
    "for i, (freq, amp) in enumerate(zip(spectral.frequencies[:5], spectral.amplitudes[:5]), 1):\n",
    "    print(f\"  {i}. Frequency: {freq:.2f}, Amplitude: {amp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spectral analysis\n",
    "visualizer = ErrorVisualizer(analyzer)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "visualizer.plot_error_signal(ax=ax1)\n",
    "visualizer.plot_spectral_analysis(ax=ax2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Polynomial Trend Detection\n",
    "\n",
    "Detect systematic bias using polynomial fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with polynomial error\n",
    "x = np.linspace(0, 10, 100)\n",
    "actual = x**2 + 2*x + 5\n",
    "predicted = x**2  # Missing linear and constant terms\n",
    "\n",
    "analyzer = ErrorPatternAnalyzer(actual, predicted, x, name=\"Polynomial Error\")\n",
    "poly = analyzer.analyze_polynomial(max_degree=5)\n",
    "\n",
    "print(f\"Detected polynomial degree: {poly.degree}\")\n",
    "print(f\"R²: {poly.r_squared:.6f}\")\n",
    "print(f\"Coefficients: {poly.coefficients}\")\n",
    "print(f\"\\nGenerated code:\")\n",
    "print(poly.to_correction_code())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Corrections\n",
    "\n",
    "Apply discovered corrections and measure improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with known error\n",
    "x = np.linspace(1, 10, 100)\n",
    "actual = np.log(x) + 5\n",
    "predicted = np.log(x)  # Missing constant\n",
    "\n",
    "analyzer = ErrorPatternAnalyzer(actual, predicted, x, name=\"Log + Constant\")\n",
    "\n",
    "print(f\"Original RMSE: {analyzer.rmse:.6f}\")\n",
    "\n",
    "# Get suggestions and apply best\n",
    "suggestions = analyzer.suggest_corrections(top_k=3)\n",
    "best = suggestions[0]\n",
    "\n",
    "print(f\"\\nApplying: {best.description}\")\n",
    "corrected = analyzer.apply_correction(best)\n",
    "\n",
    "print(f\"Corrected RMSE: {corrected.rmse:.6f}\")\n",
    "improvement = (1 - corrected.rmse / analyzer.rmse) * 100\n",
    "print(f\"Improvement: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recursive Refinement\n",
    "\n",
    "Automatically apply corrections until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with multiple error patterns\n",
    "x = np.linspace(0, 10, 150)\n",
    "actual = (np.sin(x) + \n",
    "          0.05 * x**2 + \n",
    "          0.2 * np.sin(2 * np.pi * 2.0 * x) +\n",
    "          0.1 * x)\n",
    "predicted = np.sin(x)  # Missing polynomial, harmonic, and linear\n",
    "\n",
    "analyzer = ErrorPatternAnalyzer(actual, predicted, x, name=\"Complex Error\")\n",
    "\n",
    "# Apply recursive refinement\n",
    "history = analyzer.recursive_refinement(\n",
    "    max_depth=5, \n",
    "    improvement_threshold=0.01  # Stop if improvement < 1%\n",
    ")\n",
    "\n",
    "print(f\"Initial RMSE: {history.initial_rmse:.6f}\")\n",
    "print(f\"Final RMSE:   {history.final_rmse:.6f}\")\n",
    "print(f\"Improvement:  {history.improvement:.1%}\")\n",
    "print(f\"Depth:        {history.depth} corrections\\n\")\n",
    "\n",
    "print(\"Applied corrections:\")\n",
    "for i, corr in enumerate(history.corrections_applied, 1):\n",
    "    print(f\"  {i}. {corr.description}\")\n",
    "    print(f\"     RMSE after: {history.rmse_history[i]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence\n",
    "history.plot_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scale-Dependent Errors\n",
    "\n",
    "Detect errors that depend on the magnitude of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with scale-dependent error\n",
    "x = np.linspace(1, 100, 200)\n",
    "actual = np.log(x) + 0.1 * np.sqrt(x)\n",
    "predicted = np.log(x)  # Missing sqrt term\n",
    "\n",
    "analyzer = ErrorPatternAnalyzer(actual, predicted, x, name=\"Scale-Dependent\")\n",
    "scale = analyzer.analyze_scale_dependence(n_bins=10)\n",
    "\n",
    "if scale.scale_function:\n",
    "    model = scale.scale_params.get('model', 'unknown')\n",
    "    r2 = scale.scale_params.get('r2', 0.0)\n",
    "    print(f\"Detected scale pattern: {model}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"\\nGenerated code:\")\n",
    "    print(scale.to_correction_code())\n",
    "else:\n",
    "    print(\"No significant scale dependence detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scale dependence\n",
    "visualizer = ErrorVisualizer(analyzer)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "visualizer.plot_error_signal(ax=ax1)\n",
    "visualizer.plot_scale_dependence(ax=ax2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Autocorrelation Detection\n",
    "\n",
    "Detect recursive patterns in errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with autocorrelated error\n",
    "np.random.seed(42)\n",
    "x = np.arange(200)\n",
    "# Create AR(1) error: e[t] = 0.8*e[t-1] + noise\n",
    "error_ar = np.zeros(200)\n",
    "for i in range(1, 200):\n",
    "    error_ar[i] = 0.8 * error_ar[i-1] + np.random.randn() * 0.1\n",
    "\n",
    "actual = x * 0.5 + error_ar\n",
    "predicted = x * 0.5\n",
    "\n",
    "analyzer = ErrorPatternAnalyzer(actual, predicted, x, name=\"Autocorrelated Error\")\n",
    "autocorr = analyzer.analyze_autocorrelation(max_lag=50)\n",
    "\n",
    "print(f\"Significant lags: {autocorr.significant_lags[:5]}\")\n",
    "if autocorr.ar_order:\n",
    "    print(f\"Suggested AR order: {autocorr.ar_order}\")\n",
    "    print(f\"AR coefficients: {autocorr.ar_coefficients}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize autocorrelation\n",
    "visualizer = ErrorVisualizer(analyzer)\n",
    "visualizer.plot_autocorrelation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full Dashboard\n",
    "\n",
    "Comprehensive visualization of all patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create complex error for dashboard\n",
    "x = np.linspace(0, 10, 200)\n",
    "actual = (np.sin(x) + \n",
    "          0.1 * x + \n",
    "          0.3 * np.sin(2 * np.pi * 3.0 * x))\n",
    "predicted = np.sin(x)\n",
    "\n",
    "analyzer = ErrorPatternAnalyzer(actual, predicted, x, name=\"Dashboard Example\")\n",
    "visualizer = ErrorVisualizer(analyzer)\n",
    "\n",
    "# Show full dashboard\n",
    "visualizer.plot_full_dashboard(figsize=(16, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-World Example: Zeta Zero Approximation\n",
    "\n",
    "Analyze errors in a zeta zero initial guess formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate zeta zero approximation error\n",
    "# (In practice, you'd use actual zeta zeros and your formula)\n",
    "n = np.arange(1, 101)\n",
    "\n",
    "# Simplified: actual ~ 2π(n-11/8)/log(n) + harmonics\n",
    "actual_zeros = 2 * np.pi * (n - 11/8) / np.log(n + 1e-10)\n",
    "\n",
    "# Initial guess without harmonics\n",
    "predicted_zeros = 2 * np.pi * (n - 11/8) / np.log(n + 1e-10) * 0.95  # Simplified\n",
    "\n",
    "analyzer = ErrorPatternAnalyzer(\n",
    "    actual_zeros, \n",
    "    predicted_zeros, \n",
    "    n, \n",
    "    name=\"Zeta Zero Approximation\"\n",
    ")\n",
    "\n",
    "report = analyzer.analyze_all(max_corrections=5)\n",
    "report.print_summary()\n",
    "\n",
    "print(\"\\nTop 3 correction codes:\")\n",
    "for i, sug in enumerate(report.suggestions[:3], 1):\n",
    "    print(f\"\\n{i}. {sug.description}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(sug.code_snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Corrections as Production Code\n",
    "\n",
    "Generate a complete correction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_correction_function(analyzer, suggestions, func_name=\"improved_formula\"):\n",
    "    \"\"\"Generate a complete Python function with all corrections.\"\"\"\n",
    "    code = [\n",
    "        f\"def {func_name}(x, original_prediction):\",\n",
    "        \"    import numpy as np\",\n",
    "        \"    prediction = original_prediction.copy()\",\n",
    "        \"    error = np.zeros_like(x)  # For AR models\",\n",
    "        \"\"\n",
    "    ]\n",
    "    \n",
    "    for i, suggestion in enumerate(suggestions, 1):\n",
    "        code.append(f\"    # Correction {i}: {suggestion.description}\")\n",
    "        # Indent the correction code\n",
    "        correction_lines = suggestion.code_snippet.split('\\n')\n",
    "        for line in correction_lines:\n",
    "            if line.strip():\n",
    "                code.append(f\"    {line}\")\n",
    "        code.append(\"\")\n",
    "    \n",
    "    code.append(\"    return prediction\")\n",
    "    return \"\\n\".join(code)\n",
    "\n",
    "# Generate function\n",
    "x = np.linspace(0, 10, 100)\n",
    "actual = np.sin(x) + 0.1 * x\n",
    "predicted = np.sin(x)\n",
    "\n",
    "analyzer = ErrorPatternAnalyzer(actual, predicted, x)\n",
    "suggestions = analyzer.suggest_corrections(top_k=2)\n",
    "\n",
    "correction_code = generate_correction_function(analyzer, suggestions)\n",
    "print(\"Generated production code:\")\n",
    "print(\"=\" * 70)\n",
    "print(correction_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Error Pattern Visualizer provides:\n",
    "\n",
    "1. **Automatic Pattern Discovery** - Finds patterns you might miss\n",
    "2. **Spectral Analysis** - Detects periodic corrections via FFT\n",
    "3. **Polynomial Trends** - Identifies systematic bias\n",
    "4. **Autocorrelation** - Discovers recursive patterns\n",
    "5. **Scale Dependence** - Detects x-dependent errors\n",
    "6. **Code Generation** - Produces executable correction code\n",
    "7. **Recursive Refinement** - Applies corrections until convergence\n",
    "8. **Visualization** - Comprehensive error analysis dashboards\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- Optimizing mathematical formulas (like zeta zeros)\n",
    "- Improving signal approximations\n",
    "- Analyzing model prediction errors\n",
    "- Discovering missing correction terms\n",
    "- Any scenario with actual vs predicted values\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "**Errors contain structure. This tool extracts that structure and converts it into corrections.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
